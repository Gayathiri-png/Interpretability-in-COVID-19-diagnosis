# Interpretability-in-COVID-19-diagnosis

COVID-19, declared as a pandemic by World Health Organization (WHO) and its accelerated spread  has put a stress on healthcare systems all over the world. An overwhelming ER at a hospital requires a faster patient management through faster diagnosis or observing signs of COVID-19 that could be paired up with Real Time Reverse Transcription Polymerase Chain Reaction (RT-PCR) for further effective measures. Similar to pneumonia, X-Rays for COVID cases tends to show changes in the lungs such as ground glass opacities and peripheral consolidations. Detecting the difference in densities in the chest-X-Ray images with deep neural networks can help accelerate the diagnosis process. However, deep neural networks are limited in comprehension due to their point estimate nature, forced prediction of any input, and lack of capture of uncertainty in the prediction. Visualizing the regions of abnormalities combined with uncertainty and associating them to the input pixels is critical for health care adoption. There have been several works in making point estimate deep learning networks interpretable for COVID-19, however very limited work has been found on interpreting uncertainty in a prediction and  its decomposition, to see if the uncertainty was due to the model or due to the input image. We mitigate the problem of uncertainty in predictions of COVID-19 with the help of a Bayesian Convolutional Neural Network. To address interpretability we develop a visualization framework to understand the contribution of individual features in the Chest-X-Rays to predictive uncertainty. This is further decomposed to epistemic uncertainty and aleatoric uncertainty, highlighting pixels in the Chest X-Ray  that contribute to its classification (aleatoric uncertainty), or distinguish it from the training distribution (epistemic uncertainty), together contributing to predictive uncertainty. Providing this as an assistive tool can help the radiologist understand why the model came up with a prediction and whether the regions of interest captured by the model for the specific prediction are of significance in diagnosis.

This work is developed from the foundations laid by Lucy Chai from MIT 
